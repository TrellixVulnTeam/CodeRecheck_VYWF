{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea3c5e5b-39e8-419e-aca1-5870a22a8873",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import nd\n",
    "import random\n",
    "import zipfile\n",
    "import collections\n",
    "import d2lzh as d2l\n",
    "import math\n",
    "from mxnet import autograd,gluon,nd\n",
    "from mxnet.gluon import data as gdata,loss as gloss,nn,utils as gutils\n",
    "import os\n",
    "import tarfile\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "from mxnet import init\n",
    "from mxnet.contrib import text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8dce3f0-9be0-4c0a-8773-845833ea6bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_embedding_params(filePath):\n",
    "    idx_to_token=np.load(filePath+'idx_to_token.npy')\n",
    "    #token_to_idx=np.load(filePath+'token_to_idx.npy')\n",
    "    token_to_idx={tk:idx for idx,tk in enumerate(idx_to_token)}\n",
    "    embedding_weight=nd.load(filePath+'embedding_weight.txt')\n",
    "    return idx_to_token,token_to_idx,embedding_weight\n",
    "def save_params(params,filePath):\n",
    "    file=filePath\n",
    "    nd.save(file+'conv_layer_one_W.txt',params[0])\n",
    "    nd.save(file+'conv_layer_one_B.txt',params[1])\n",
    "    nd.save(file+'conv_layer_two_W.txt',params[2])\n",
    "    nd.save(file+'conv_layer_two_B.txt',params[3])\n",
    "    nd.save(file+'conv_layer_three_W.txt',params[4])\n",
    "    nd.save(file+'conv_layer_three_B.txt',params[5])\n",
    "    nd.save(file+'conv_layer_four_W.txt',params[6])\n",
    "    nd.save(file+'conv_layer_four_B.txt',params[7])\n",
    "    nd.save(file+'ful_layer_connect_W.txt',params[8])\n",
    "    nd.save(file+'ful_layer_connect_B.txt',params[9])\n",
    "    return\n",
    "def init_params():\n",
    "    conv_one_W=nd.zeros(shape=(20,3))\n",
    "    conv_one_W=nd.random.normal(scale=0.01,shape=(conv_one_W.shape))\n",
    "    conv_one_B=nd.zeros(shape=(3,3))\n",
    "    conv_one_B=nd.random.normal(scale=0.01,shape=(conv_one_B.shape))\n",
    "    \n",
    "    conv_two_W=nd.zeros(shape=(9,3))\n",
    "    conv_two_W=nd.random.normal(scale=0.01,shape=(conv_two_W.shape))\n",
    "    conv_two_B=nd.zeros(shape=(3,3))\n",
    "    conv_two_B=nd.random.normal(scale=0.01,shape=(conv_two_B.shape))\n",
    "\n",
    "    conv_three_W=nd.zeros(shape=(9,3))\n",
    "    conv_three_W=nd.random.normal(scale=0.01,shape=(conv_three_W.shape))\n",
    "    conv_three_B=nd.zeros(shape=(3,3))\n",
    "    conv_three_B=nd.random.normal(scale=0.01,shape=(conv_three_B.shape))\n",
    "\n",
    "    conv_four_W=nd.zeros(shape=(9,1))\n",
    "    conv_four_W=nd.random.normal(scale=0.01,shape=(conv_four_W.shape))\n",
    "    conv_four_B=nd.zeros(1)\n",
    "    conv_four_B=nd.random.normal(scale=0.01,shape=(conv_four_B.shape))\n",
    "\n",
    "    full_connect_W=nd.zeros(shape=(100,80))\n",
    "    full_connect_W=nd.random.normal(scale=0.01,shape=(full_connect_W.shape))\n",
    "    full_connect_B=nd.zeros(shape=(80,1))\n",
    "    full_connect_B=nd.random.normal(scale=0.01,shape=(full_connect_B.shape))\n",
    "        #print(conv_one_W)\n",
    "    params=[conv_one_W,conv_one_B,conv_two_W,conv_two_B,conv_three_W,conv_three_B,conv_four_W,conv_four_B,full_connect_W,full_connect_B]\n",
    "    return params\n",
    "def read_file_data(filePath,token_to_idx,text_size):\n",
    "    f=open(filePath)\n",
    "    lines=f.readlines()\n",
    "    raw_dataset=[st.split() for st in lines]\n",
    "    raw_dataset.remove([])\n",
    "    wordNum=0;\n",
    "    dataset_temp=[]\n",
    "    for st in raw_dataset:\n",
    "         for wt in st:\n",
    "                wordNum+=1\n",
    "    dataset=[[token_to_idx[tk] for tk in st if tk in token_to_idx] for st in raw_dataset]\n",
    "    dict_size=text_size #文本大小\n",
    "    dataset=np.array(dataset)\n",
    "    temp_one=np.array([])\n",
    "    for row in dataset:\n",
    "        for col in row:\n",
    "            temp_one=np.append(temp_one,np.array(col))\n",
    "    zeros=np.zeros(dict_size-wordNum)\n",
    "    temp_two=np.append(temp_one,zeros)\n",
    "    temp_three=np.array(temp_two)\n",
    "    #print(temp_three)\n",
    "    temp_three=temp_three.reshape(dict_size,-1)\n",
    "    return temp_three\n",
    "def read_mult_input_data(fileDir,token_to_idx,text_size):\n",
    "    positive_text=read_file_data(fileDir+'positiveText/positiveText_cnn.lex',token_to_idx,text_size)\n",
    "    original_text=read_file_data(fileDir+'originalText/originalText_cnn.lex',token_to_idx,text_size)\n",
    "    negative_text=read_file_data(fileDir+'negativeText/negativeText_cnn.lex',token_to_idx,text_size)\n",
    "    return original_text,positive_text,negative_text\n",
    "def read_params(filePath):\n",
    "    layer_one_W=nd.load(filePath+'conv_layer_one_W.txt')\n",
    "    layer_one_B=nd.load(filePath+'conv_layer_one_B.txt')\n",
    "    layer_two_W=nd.load(filePath+'conv_layer_two_W.txt')\n",
    "    layer_two_B=nd.load(filePath+'conv_layer_two_B.txt')\n",
    "    layer_three_W=nd.load(filePath+'conv_layer_three_W.txt')\n",
    "    layer_three_B=nd.load(filePath+'conv_layer_three_B.txt')\n",
    "    layer_four_W=nd.load(filePath+'conv_layer_four_W.txt')\n",
    "    layer_four_B=nd.load(filePath+'conv_layer_four_B.txt')\n",
    "    layer_connect_W=nd.load(filePath+'ful_layer_connect_W.txt')\n",
    "    layer_connect_B=nd.load(filePath+'ful_layer_connect_B.txt')\n",
    "    params=[layer_one_W,layer_one_B,layer_two_W,layer_two_B,layer_three_W,layer_three_B,layer_four_W,layer_four_B,layer_connect_W,layer_connect_B]\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78848542-bdb7-4997-9dd2-ca459e7591b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_token,token_to_idx,embedding_weight=read_embedding_params(\"data/params/word2Vec/\")\n",
    "embedding_dim=20\n",
    "text_size=800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fb4d2a5-e114-401f-b188-f7469ff605a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#params=init_params()\n",
    "params=read_params('data/params/conv/')\n",
    "#params_two=params[0]\n",
    "#print(params[0][0])\n",
    "conv_one_W=params[0][0]\n",
    "conv_one_B=params[1][0]\n",
    "conv_two_W=params[2][0]\n",
    "conv_two_B=params[3][0]\n",
    "conv_three_W=params[4][0]\n",
    "conv_three_B=params[5][0]\n",
    "conv_four_W=params[6][0]\n",
    "conv_four_B=params[7][0]\n",
    "full_connect_W=params[8][0]\n",
    "full_connect_B=params[9][0]\n",
    "params=[conv_one_W,conv_one_B,conv_two_W,conv_two_B,conv_three_W,conv_three_B,conv_four_W,conv_four_B,full_connect_W,full_connect_B]\n",
    "#print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86c1656f-33b7-4edf-9627-e0f7624d61f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_weight=nd.array(embedding_weight[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93ed9de-5480-4d57-a31a-dfacfa278ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_params(params,'data/params/conv/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63a08c58-a7d0-4fbe-9935-d3ffef482dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_activation(w,b,z,ctx):\n",
    "    w=w.as_in_context(ctx)\n",
    "    b=b.as_in_context(ctx)\n",
    "    sum=0;\n",
    "    for t in z:\n",
    "        sum+=t.sum()\n",
    "    if sum==0:\n",
    "        return 0\n",
    "    #sum_two=(nd.dot(z,w)+b,ctx=ctx)\n",
    "    #sum_two=sum_two.as_in_context(ctx)\n",
    "    sum_two=nd.dot(z,w)+b\n",
    "    for i in range(sum_two.shape[0]):\n",
    "        for j in range(sum_two.shape[1]):\n",
    "            sum_two[i][j]=max(sum_two[i][j],0)\n",
    "   \n",
    "    return sum_two\n",
    "def conv_layer(X,windows_size,param_w,param_b,ctx):\n",
    "    #windows_size=3\n",
    "    #print(X.shape[0])\n",
    "    Y=nd.zeros((X.shape[0],windows_size,param_w.shape[1]))\n",
    "    Y=Y.as_in_context(ctx)\n",
    "    X=X.as_in_context(ctx)\n",
    "    param_w=param_w.as_in_context(ctx)\n",
    "    params_b=param_b.as_in_context(ctx)\n",
    "    for i in range(X.shape[0]):\n",
    "        if(i+windows_size>=X.shape[0]):\n",
    "            break\n",
    "        Y[i]=conv_activation(param_w,param_b,X[i:i+windows_size],ctx)\n",
    "    return Y\n",
    "    \n",
    "def myMax(X_one,X_two):\n",
    "    sum_one=X_one.sum()\n",
    "    sum_two=X_two.sum()\n",
    "    if sum_one>sum_two:\n",
    "        return X_one\n",
    "    else :\n",
    "        return X_two\n",
    "def max_pool_layer(X,ctx):\n",
    "    temp= int(X.shape[0]/2)\n",
    "    Y=nd.zeros((temp,X.shape[1]))\n",
    "    Y=Y.as_in_context(ctx)\n",
    "    j=0\n",
    "    for i in range(X.shape[0]-1):\n",
    "        if i%2==0:\n",
    "            Y[j]=myMax(X[i],X[i+1])\n",
    "            j+=1\n",
    "    return Y\n",
    "def full_connect_layer(X,W,B,ctx):\n",
    "    Y=nd.zeros(W.shape[1])\n",
    "    Y=Y.as_in_context(ctx)\n",
    "    X=X.as_in_context(ctx)\n",
    "    W=W.as_in_context(ctx)\n",
    "    B=B.as_in_context(ctx)\n",
    "    Y=nd.dot(X.T,W).T+B\n",
    "    return Y.tanh()\n",
    "\n",
    "def dropout_layer(X,drop_prob):\n",
    "    assert 0 <= drop_prob <=1\n",
    "    keep_prob=1-drop_prob\n",
    "    if keep_prob==0:\n",
    "        return X.zeros_like()\n",
    "    mask=nd.random.uniform(0,1,X.shape)<keep_prob\n",
    "    return mask*X/keep_prob\n",
    "def make_parameterDict(params):\n",
    "    conv_one_W,conv_one_B,conv_two_W,conv_two_B,conv_three_W,conv_three_B,conv_four_W,conv_four_B,full_connect_W,full_connect_B=params\n",
    "    paraDict=gluon.ParameterDict()\n",
    "    \n",
    "    para_one_W=gluon.Parameter('conv_one_W',shape=conv_one_W.shape,init=init.Xavier())\n",
    "    para_one_B=gluon.Parameter('conv_one_B',shape=conv_one_B.shape,init=init.Xavier())\n",
    "    para_one_W.initialize()\n",
    "    para_one_B.initialize()\n",
    "    paraDict._params['conv_one_W']=para_one_W\n",
    "    paraDict._params['conv_one_B']=para_one_B\n",
    "        \n",
    "    para_two_W=gluon.Parameter('conv_two_W',shape=conv_two_W.shape,init=init.Xavier())\n",
    "    para_two_B=gluon.Parameter('conv_two_B',shape=conv_two_B.shape,init=init.Xavier())\n",
    "    para_two_W.initialize()\n",
    "    para_two_B.initialize()\n",
    "    paraDict._params['conv_two_W']=para_two_W\n",
    "    paraDict._params['conv_two_B']=para_two_B\n",
    "        \n",
    "    para_three_W=gluon.Parameter('conv_three_W',shape=conv_three_W.shape,init=init.Xavier())\n",
    "    para_three_B=gluon.Parameter('conv_three_B',shape=conv_three_B.shape,init=init.Xavier())\n",
    "    para_three_W.initialize()\n",
    "    para_three_B.initialize()\n",
    "    paraDict._params['conv_three_W']=para_three_W\n",
    "    paraDict._params['conv_three_B']=para_three_B\n",
    "        \n",
    "    para_four_W=gluon.Parameter('conv_four_W',shape=conv_four_W.shape,init=init.Xavier())\n",
    "    para_four_B=gluon.Parameter('conv_four_B',shape=conv_four_B.shape)\n",
    "    para_four_W.initialize()\n",
    "    para_four_B.initialize()\n",
    "    paraDict._params['conv_four_W']=para_four_W\n",
    "    paraDict._params['conv_four_B']=para_four_B\n",
    "        \n",
    "    para_full_connect_W=gluon.Parameter('full_connect_W',shape=full_connect_W.shape,init=init.Xavier())\n",
    "    para_full_connect_B=gluon.Parameter('full_connect_B',shape=full_connect_B.shape,init=init.Xavier())\n",
    "    para_full_connect_W.initialize()\n",
    "    para_full_connect_B.initialize()\n",
    "    paraDict._params['full_connect_W']=para_full_connect_W\n",
    "    paraDict._params['full_connect_B']=para_full_connect_B\n",
    "    return paraDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d179a02-14db-48fa-9d20-f1800e9019b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNCodeRecheckTrainModel(nn.Block):\n",
    "    def __init__(self,vocab_size,embed_size,embedding_weight,**kwargs):\n",
    "        super(CNNCodeRecheckTrainModel,self).__init__(**kwargs)\n",
    "        #参与训练嵌入层\n",
    "        self.embedding=nn.Embedding(input_dim=vocab_size,output_dim=embed_size)\n",
    "        self.embedding.weight.initialize(init=init.Xavier(),force_reinit=True)\n",
    "        self.embedding.weight.set_data(embedding_weight)\n",
    "        #不参与训练嵌入层\n",
    "        #self.constant_embedding=nn.Embedding(input_dim=vocab_size,output_dim=embed_size)\n",
    "        #申请梯度\n",
    "        params=read_params('data/params/conv/')\n",
    "        conv_one_W=params[0][0]\n",
    "        conv_one_B=params[1][0]\n",
    "        conv_two_W=params[2][0]\n",
    "        conv_two_B=params[3][0]\n",
    "        conv_three_W=params[4][0]\n",
    "        conv_three_B=params[5][0]\n",
    "        conv_four_W=params[6][0]\n",
    "        conv_four_B=params[7][0]\n",
    "        full_connect_W=params[8][0]\n",
    "        full_connect_B=params[9][0]\n",
    "        params=[conv_one_W,conv_one_B,conv_two_W,conv_two_B,conv_three_W,conv_three_B,conv_four_W,conv_four_B,full_connect_W,full_connect_B]\n",
    "        for param in params:\n",
    "            #print(param)\n",
    "            param.attach_grad()\n",
    "        ctx=d2l.try_gpu()\n",
    "        #self.embedding=self.embedding.as_in_context(ctx)\n",
    "        self.conv_one_W=conv_one_W\n",
    "        self.conv_one_W=self.conv_one_W.as_in_context(ctx)\n",
    "        #print(self.conv_one_W)\n",
    "        self.conv_one_B=conv_one_B\n",
    "        self.conv_one_B=self.conv_one_B.as_in_context(ctx)\n",
    "        #print(self.conv_one_B)\n",
    "        self.conv_two_W=conv_two_W\n",
    "        self.conv_two_W=self.conv_two_W.as_in_context(ctx)\n",
    "        \n",
    "        self.conv_two_B=conv_two_B\n",
    "        self.conv_two_B=self.conv_two_B.as_in_context(ctx)\n",
    "        \n",
    "        self.conv_three_W=conv_three_W\n",
    "        self.conv_three_W=self.conv_three_W.as_in_context(ctx)\n",
    "        \n",
    "        self.conv_three_B=conv_three_B\n",
    "        self.conv_three_B=self.conv_three_B.as_in_context(ctx)\n",
    "        \n",
    "        self.conv_four_W=conv_four_W\n",
    "        self.conv_four_W=self.conv_four_W.as_in_context(ctx)\n",
    "        \n",
    "        self.conv_four_B=conv_four_B\n",
    "        self.conv_four_B=self.conv_four_B.as_in_context(ctx)\n",
    "        \n",
    "        self.full_connect_W=full_connect_W\n",
    "        self.full_connect_W=self.full_connect_W.as_in_context(ctx)\n",
    "        \n",
    "        self.full_connect_B=full_connect_B\n",
    "        self.full_connect_B=self.full_connect_B.as_in_context(ctx)\n",
    "        self.conv_layer=conv_layer\n",
    "        self.max_pool_layer=max_pool_layer\n",
    "        self.full_connect_layer=full_connect_layer\n",
    "        self.dropout_layer=dropout_layer\n",
    "        self.paraDict=make_parameterDict(params)\n",
    "    def coll_para():\n",
    "        print(paraDict)\n",
    "        #return paraDict    \n",
    "    def set_embedding_weight(self,embedding_weight):\n",
    "        self.embedding.weight.set_data(embedding_weight)\n",
    "    def forward(self,inputs,mode='train'):\n",
    "        ctx=d2l.try_gpu()\n",
    "        inputs_embeddings=nd.array([],ctx=ctx)\n",
    "        if mode=='train':\n",
    "            inputs_embeddings=nd.stack(nd.flatten(self.embedding(inputs[0])),nd.flatten(self.embedding(inputs[1])),nd.flatten(self.embedding(inputs[2])))\n",
    "        else :\n",
    "            inputs_embeddings=nd.stack(nd.flatten(self.embedding(inputs[0])),nd.flatten(self.embedding(inputs[1])))\n",
    "        #print(inputs_embeddings)\n",
    "        #input_embedding=inputs_embeddings[0]\n",
    "        #sum=0;\n",
    "        results=[]\n",
    "        for input_embedding in inputs_embeddings:\n",
    "            input_embedding=input_embedding.as_in_context(ctx)\n",
    "            result=conv_layer(input_embedding,3,self.conv_one_W,self.conv_one_B,ctx)\n",
    "            result=result.as_in_context(ctx)\n",
    "            result=nd.flatten(result)\n",
    "            print(result)\n",
    "            result=max_pool_layer(result,ctx)\n",
    "            result=conv_layer(result,3,self.conv_two_W,self.conv_two_B,ctx)\n",
    "            result=nd.flatten(result)\n",
    "            result=max_pool_layer(result,ctx)\n",
    "            print(result)\n",
    "            result=conv_layer(result,3,self.conv_three_W,self.conv_three_B,ctx)\n",
    "            result=nd.flatten(result)\n",
    "            result=max_pool_layer(result,ctx)\n",
    "            print(result)\n",
    "            result=conv_layer(result,1,self.conv_four_W,self.conv_four_B,ctx)\n",
    "            result=nd.flatten(result)\n",
    "            result=result.as_in_context(ctx)\n",
    "            result=full_connect_layer(result,self.full_connect_W,self.full_connect_B,ctx)\n",
    "            print(result)\n",
    "            results.append(result)\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "722dc3b5-9d2d-49d6-b18e-451f7d032664",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mult_file_data(sourcePath,negativePath,positivePath,token_to_idx,text_size):\n",
    "    text=[]\n",
    "    sourceText=read_file_data(sourcePath,token_to_idx,text_size)\n",
    "    negativeText=read_file_data(negativePath,token_to_idx,text_size)\n",
    "    positiveText=read_file_data(positivePath,token_to_idx,text_size)\n",
    "    sourceText=nd.array(sourceText)\n",
    "    positiveText=nd.array(positiveText)\n",
    "    negativeText=nd.array(negativeText)\n",
    "    text=nd.stack(sourceText,positiveText,negativeText)\n",
    "    return text\n",
    "def get_fileNames(dirPath):\n",
    "    filenameSet=[]\n",
    "    for dirpath,dirnames,filenames in os.walk(dirPath):\n",
    "        for fileaname in filenames:\n",
    "            filePath=os.path.join(dirpath,fileaname)\n",
    "            if os.path.isfile(filePath):\n",
    "                if filePath.endswith(\".txt\"):\n",
    "                       filenameSet.append(fileaname)\n",
    "    #print(filenameSet)\n",
    "    sorted(filenameSet)\n",
    "    return filenameSet\n",
    "def get_all_file_in_dir(dirPath):\n",
    "    filePaths=[]\n",
    "    for dirpath,dirnames,filenames in os.walk(dirPath):\n",
    "        for filename in filenames:\n",
    "           # filenameSet.append(filenames)\n",
    "            #print(os.path.join(dirpath,filename))\n",
    "            filePath=os.path.join(dirpath,filename)\n",
    "            if os.path.isfile(filePath):\n",
    "                if filePath.endswith(\".txt\"):\n",
    "                    #print(filePath)\n",
    "                    filePaths.append(filePath)\n",
    "    sorted(filePaths)\n",
    "    return filePaths\n",
    "def load_train_data(net,token_to_idx,text_size):\n",
    "    #filenames=get_fileNames(\"C:/Users/lwq/Desktop/trainData/source\")\n",
    "    #print(filenames)\n",
    "    sourceFiles=get_all_file_in_dir(\"C:/Users/lwq/Desktop/trainData/source\")\n",
    "    positiveFiles=get_all_file_in_dir(\"C:/Users/lwq/Desktop/trainData/positive\")\n",
    "    negativeFiles=get_all_file_in_dir(\"C:/Users/lwq/Desktop/trainData/negative\")\n",
    "    texts=[]\n",
    "    for idx in range(len(sourceFiles)):\n",
    "        #print(sourceFiles[idx])\n",
    "        #print(negativeFiles[idx])\n",
    "        #print(positiveFiles[idx])\n",
    "        text=get_mult_file_data(sourceFiles[idx],negativeFiles[idx],positiveFiles[idx],token_to_idx,text_size)\n",
    "        #print(text)\n",
    "        texts.append(text)\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "60397055-6486-40e4-8a7c-ce1ea353c843",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_text,positive_text,negative_text=read_mult_input_data(\"data/input_data/\",token_to_idx,text_size)\n",
    "original_text=nd.array(original_text)\n",
    "positive_text=nd.array(positive_text)\n",
    "negative_text=nd.array(negative_text)\n",
    "text=nd.stack(original_text,positive_text,negative_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5e513947-2851-49d6-b6c8-0f7d787ca14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr,num_epochs=0.001,5\n",
    "net=CNNCodeRecheckTrainModel(len(idx_to_token),embedding_dim,embedding_weight)\n",
    "trainer=gluon.Trainer(net.paraDict,'adam',{'learning_rate':lr})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6a2756-c82b-4b9d-940c-54056e0cf1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start=time.time()\n",
    "y=net(text)\n",
    "time_end=time.time()\n",
    "print('花费时间%.3fs'%(time_end-time_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45efbc9f-b11e-4803-96a0-d64da327fde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "net=CNNCodeRecheckTrainModel(len(idx_to_token),embedding_dim,embedding_weight)\n",
    "texts=load_train_data(net,token_to_idx,text_size)\n",
    "text_embeddings=[]\n",
    "sum=0\n",
    "time_total=0\n",
    "for text in texts:\n",
    "    time_start=time.time()\n",
    "    embedd=net(text)\n",
    "    text_embeddings.append(embedd)\n",
    "    time_end=time.time()\n",
    "    cost_time=time_end-time_start\n",
    "    time_total+=cost_time\n",
    "    print('样本%d计算花费时间%.3fs'%(sum,cost_time))\n",
    "    sum+=1\n",
    "print('%d份样本花费时间%.3fs'%(sum,time_total))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
