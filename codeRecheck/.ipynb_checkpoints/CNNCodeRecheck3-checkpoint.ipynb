{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "923daae8-ff0f-4860-ad0f-4ae00f1322b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import nd\n",
    "import random\n",
    "import zipfile\n",
    "import collections\n",
    "import d2lzh as d2l\n",
    "import math\n",
    "from mxnet import autograd,gluon,nd\n",
    "from mxnet.gluon import data as gdata,loss as gloss,nn,utils as gutils\n",
    "import os\n",
    "import tarfile\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "from mxnet import init\n",
    "from mxnet.contrib import text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a429fa2a-33d9-4912-9ba4-7da584b40a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_embedding_params(filePath):\n",
    "    idx_to_token=np.load(filePath+'idx_to_token.npy')\n",
    "    #token_to_idx=np.load(filePath+'token_to_idx.npy')\n",
    "    token_to_idx={tk:idx for idx,tk in enumerate(idx_to_token)}\n",
    "    embedding_weight=nd.load(filePath+'embedding_weight.txt')\n",
    "    return idx_to_token,token_to_idx,embedding_weight\n",
    "def save_params(params,filePath):\n",
    "    file=filePath\n",
    "    nd.save(file+'conv_layer_one_W.txt',params[0])\n",
    "    nd.save(file+'conv_layer_one_B.txt',params[1])\n",
    "    nd.save(file+'conv_layer_two_W.txt',params[2])\n",
    "    nd.save(file+'conv_layer_two_B.txt',params[3])\n",
    "    nd.save(file+'conv_layer_three_W.txt',params[4])\n",
    "    nd.save(file+'conv_layer_three_B.txt',params[5])\n",
    "    nd.save(file+'conv_layer_four_W.txt',params[6])\n",
    "    nd.save(file+'conv_layer_four_B.txt',params[7])\n",
    "    nd.save(file+'ful_layer_connect_W.txt',params[8])\n",
    "    nd.save(file+'ful_layer_connect_B.txt',params[9])\n",
    "    return\n",
    "def init_params():\n",
    "    conv_one_W=nd.zeros(shape=(20,3))\n",
    "    conv_one_W=nd.random.normal(scale=0.01,shape=(conv_one_W.shape))\n",
    "    conv_one_B=nd.zeros(shape=(3,3))\n",
    "    conv_one_B=nd.random.normal(scale=0.01,shape=(conv_one_B.shape))\n",
    "    \n",
    "    conv_two_W=nd.zeros(shape=(9,3))\n",
    "    conv_two_W=nd.random.normal(scale=0.01,shape=(conv_two_W.shape))\n",
    "    conv_two_B=nd.zeros(shape=(3,3))\n",
    "    conv_two_B=nd.random.normal(scale=0.01,shape=(conv_two_B.shape))\n",
    "\n",
    "    conv_three_W=nd.zeros(shape=(9,3))\n",
    "    conv_three_W=nd.random.normal(scale=0.01,shape=(conv_three_W.shape))\n",
    "    conv_three_B=nd.zeros(shape=(3,3))\n",
    "    conv_three_B=nd.random.normal(scale=0.01,shape=(conv_three_B.shape))\n",
    "\n",
    "    conv_four_W=nd.zeros(shape=(9,1))\n",
    "    conv_four_W=nd.random.normal(scale=0.01,shape=(conv_four_W.shape))\n",
    "    conv_four_B=nd.zeros(1)\n",
    "    conv_four_B=nd.random.normal(scale=0.01,shape=(conv_four_B.shape))\n",
    "\n",
    "    full_connect_W=nd.zeros(shape=(100,80))\n",
    "    full_connect_W=nd.random.normal(scale=0.01,shape=(full_connect_W.shape))\n",
    "    full_connect_B=nd.zeros(shape=(80,1))\n",
    "    full_connect_B=nd.random.normal(scale=0.01,shape=(full_connect_B.shape))\n",
    "        #print(conv_one_W)\n",
    "    params=[conv_one_W,conv_one_B,conv_two_W,conv_two_B,conv_three_W,conv_three_B,conv_four_W,conv_four_B,full_connect_W,full_connect_B]\n",
    "    return params\n",
    "def read_file_data(filePath,token_to_idx,text_size):\n",
    "    f=open(filePath)\n",
    "    lines=f.readlines()\n",
    "    raw_dataset=[st.split() for st in lines]\n",
    "    raw_dataset.remove([])\n",
    "    wordNum=0;\n",
    "    dataset_temp=[]\n",
    "    for st in raw_dataset:\n",
    "         for wt in st:\n",
    "                wordNum+=1\n",
    "    dataset=[[token_to_idx[tk] for tk in st if tk in token_to_idx] for st in raw_dataset]\n",
    "    dict_size=text_size #文本大小\n",
    "    dataset=np.array(dataset)\n",
    "    temp_one=np.array([])\n",
    "    count=0;\n",
    "    for row in dataset:\n",
    "        for col in row:\n",
    "            temp_one=np.append(temp_one,np.array(col))\n",
    "            count+=1\n",
    "            if(count>=text_size):\n",
    "                break\n",
    "        if(count>=text_size):\n",
    "            break\n",
    "    array_size=0\n",
    "    if (dict_size-wordNum)>=0:\n",
    "        array_size=dict_size-wordNum\n",
    "    zeros=np.zeros(array_size)\n",
    "    temp_two=np.append(temp_one,zeros)\n",
    "    temp_three=np.array(temp_two)\n",
    "    #print(temp_three)\n",
    "    temp_three=temp_three.reshape(dict_size,-1)\n",
    "    return temp_three\n",
    "def read_mult_input_data(fileDir,token_to_idx,text_size):\n",
    "    positive_text=read_file_data(fileDir+'positiveText/positiveText_cnn.lex',token_to_idx,text_size)\n",
    "    original_text=read_file_data(fileDir+'originalText/originalText_cnn.lex',token_to_idx,text_size)\n",
    "    negative_text=read_file_data(fileDir+'negativeText/negativeText_cnn.lex',token_to_idx,text_size)\n",
    "    return original_text,positive_text,negative_text\n",
    "def read_params(filePath):\n",
    "    layer_one_W=nd.load(filePath+'conv_layer_one_W.txt')\n",
    "    layer_one_B=nd.load(filePath+'conv_layer_one_B.txt')\n",
    "    layer_two_W=nd.load(filePath+'conv_layer_two_W.txt')\n",
    "    layer_two_B=nd.load(filePath+'conv_layer_two_B.txt')\n",
    "    layer_three_W=nd.load(filePath+'conv_layer_three_W.txt')\n",
    "    layer_three_B=nd.load(filePath+'conv_layer_three_B.txt')\n",
    "    layer_four_W=nd.load(filePath+'conv_layer_four_W.txt')\n",
    "    layer_four_B=nd.load(filePath+'conv_layer_four_B.txt')\n",
    "    layer_connect_W=nd.load(filePath+'ful_layer_connect_W.txt')\n",
    "    layer_connect_B=nd.load(filePath+'ful_layer_connect_B.txt')\n",
    "    params=[layer_one_W,layer_one_B,layer_two_W,layer_two_B,layer_three_W,layer_three_B,layer_four_W,layer_four_B,layer_connect_W,layer_connect_B]\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "f7cfe4b9-ed96-4b5f-8c39-77c0fefc03d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_token,token_to_idx,embedding_weight=read_embedding_params(\"data/params/word2Vec/\")\n",
    "embedding_dim=20\n",
    "text_size=816\n",
    "embedding_weight=nd.array(embedding_weight[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "7de6dc8f-d85a-4e59-9589-e9d2f1ddf4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#params=init_params()\n",
    "params=read_params('data/params/conv/')\n",
    "#params_two=params[0]\n",
    "#print(params[0][0])\n",
    "conv_one_W=params[0][0].as_in_context(ctx)\n",
    "conv_one_B=params[1][0].as_in_context(ctx)\n",
    "conv_two_W=params[2][0].as_in_context(ctx)\n",
    "conv_two_B=params[3][0].as_in_context(ctx)\n",
    "conv_three_W=params[4][0].as_in_context(ctx)\n",
    "conv_three_B=params[5][0].as_in_context(ctx)\n",
    "conv_four_W=params[6][0].as_in_context(ctx)\n",
    "conv_four_B=params[7][0].as_in_context(ctx)\n",
    "full_connect_W=params[8][0].as_in_context(ctx)\n",
    "full_connect_B=params[9][0].as_in_context(ctx)\n",
    "params=[conv_one_W,conv_one_B,conv_two_W,conv_two_B,conv_three_W,conv_three_B,conv_four_W,conv_four_B,full_connect_W,full_connect_B]\n",
    "#print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713af13c-4ac8-47d0-852e-60eb032598d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_params(params,'data/params/conv/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df795b66-75a3-4036-be52-69cecc123d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_text,positive_text,negative_text=read_mult_input_data(\"data/input_data/\",token_to_idx,text_size)\n",
    "\n",
    "original_text=nd.array(original_text,ctx=ctx)\n",
    "positive_text=nd.array(positive_text,ctx=ctx)\n",
    "negative_text=nd.array(negative_text,ctx=ctx)\n",
    "text=nd.stack(original_text,positive_text,negative_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "c40cbda7-53b5-478e-a2ab-b3e20152a3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_activation(w,b,z,ctx):\n",
    "    \n",
    "    #print(\"temp_z[0]%s:\"%(temp_z[0]))\n",
    "    #if isinstance(z,list):\n",
    "    #    if len(z)==3:\n",
    "    #        temp_z=nd.stack(z[0],z[1],z[2])\n",
    "    #        temp_z=nd.flatten(temp_z)\n",
    "    #    elif len(z)==1:\n",
    "    #        temp_z=z[0]\n",
    "    \n",
    "    sum=z.sum()\n",
    "    if sum==0:\n",
    "        return 0\n",
    "    #sum_two=(nd.dot(z,w)+b,ctx=ctx)\n",
    "    #sum_two=sum_two.as_in_context(ctx)\n",
    "    #sum_two=nd.array((temp_z.shape[0],w.shape[1]),ctx=ctx)\n",
    "    sum_two=nd.dot(z,w)+b\n",
    "    #print(sum_two)\n",
    "    #sum_three=nd.zeros((sum_two.shape[0],sum_two.shape[1]),ctx=ctx)\n",
    "    #sum_three=\n",
    "    #for i in range(sum_two.shape[0]):\n",
    "    #    for j in range(sum_two.shape[1]):\n",
    "     #       #sum_three[i][j]=max(sum_two[i][j],0)\n",
    "     #       if sum_two[i][j]<0:\n",
    "      #          sum_three[i][j]=0\n",
    "      #      else:\n",
    "       #         sum_three[i][j]=sum_two[i][j]\n",
    "    return nd.relu(sum_two)\n",
    "def conv_layer(X,windows_size,param_w,param_b,ctx):\n",
    "    size=X.shape[0]\n",
    "    Y=np.array([])\n",
    "    for i in range(size):\n",
    "        if(i+windows_size>=size):\n",
    "            break\n",
    "        temp=conv_activation(param_w,param_b,X[i:i+windows_size],ctx)\n",
    "        temp=temp.asnumpy()\n",
    "        if i==0:\n",
    "            Y=temp\n",
    "        else:\n",
    "            Y=np.append(Y,temp,axis=0)\n",
    "    for x in range(windows_size):\n",
    "        temp=nd.zeros((windows_size,param_w.shape[1]),ctx=ctx)\n",
    "        temp=temp.asnumpy()\n",
    "        Y=np.append(Y,temp,axis=0)\n",
    "    return nd.array(Y,ctx=ctx)\n",
    "    \n",
    "def myMax(X_one,X_two):\n",
    "    sum_one=X_one.sum()\n",
    "    sum_two=X_two.sum()\n",
    "    if sum_one>sum_two:\n",
    "        return X_one\n",
    "    else :\n",
    "        return X_two\n",
    "def max_pool_layer(X,ctx):\n",
    "    #temp= int(len(X)/2)\n",
    "    Y=np.array([])\n",
    "    j=0\n",
    "    for i in range(X.shape[0]-1):\n",
    "        if i%2==0:\n",
    "            temp=myMax(X[i],X[i+1])\n",
    "            temp=temp.asnumpy()\n",
    "            if i==0:\n",
    "                Y=temp\n",
    "            else :\n",
    "                Y=np.append(Y,temp,axis=0)\n",
    "            j+=1\n",
    "    #print(Y)\n",
    "    return nd.array(Y,ctx=ctx)\n",
    "def full_connect_layer(X,W,B,ctx):\n",
    "    #print(X)\n",
    "    #temp_X=np.array(X)\n",
    "    #temp_X=[]\n",
    "    #for x in X:\n",
    "    #    temp_X.append(x[0])\n",
    "    #print(\"temp_X\")\n",
    "    #print(temp_X)\n",
    "    #temp=np.array(temp_X)\n",
    "    #print(\"temp\")\n",
    "    #print(temp)\n",
    "    #Y=nd.zeros(W.shape[1],ctx=ctx)\n",
    "    Y=nd.dot(X.T,W).T+B\n",
    "    return Y.tanh()\n",
    "\n",
    "def dropout_layer(X,drop_prob):\n",
    "    assert 0 <= drop_prob <=1\n",
    "    keep_prob=1-drop_prob\n",
    "    if keep_prob==0:\n",
    "        return X.zeros_like()\n",
    "    mask=nd.random.uniform(0,1,X.shape)<keep_prob\n",
    "    return mask*X/keep_prob\n",
    "def make_parameterDict(params):\n",
    "    conv_one_W,conv_one_B,conv_two_W,conv_two_B,conv_three_W,conv_three_B,conv_four_W,conv_four_B,full_connect_W,full_connect_B=params\n",
    "    paraDict=gluon.ParameterDict()\n",
    "    ctx=d2l.try_all_gpus()\n",
    "    #paraDict=paraDict.as_in_contect(ctx)\n",
    "    para_one_W=gluon.Parameter('conv_one_W',shape=conv_one_W.shape)\n",
    "    para_one_B=gluon.Parameter('conv_one_B',shape=conv_one_B.shape)\n",
    "    #para_one_W.initialize()\n",
    "    #para_one_B.initialize()\n",
    "    para_one_W._init_impl(conv_one_W,ctx)\n",
    "    para_one_B._init_impl(conv_one_B,ctx)\n",
    "    #para_one_W.weight=conv_one_W\n",
    "    #para_one_B.wegiht=conv_one_B\n",
    "    \n",
    "    paraDict._params['conv_one_W']=para_one_W\n",
    "    paraDict._params['conv_one_B']=para_one_B\n",
    "        \n",
    "    para_two_W=gluon.Parameter('conv_two_W',shape=conv_two_W.shape,init=init.Xavier())\n",
    "    para_two_B=gluon.Parameter('conv_two_B',shape=conv_two_B.shape,init=init.Xavier())\n",
    "    #para_two_W.initialize()\n",
    "    #para_two_B.initialize()\n",
    "    para_two_W._init_impl(conv_two_W,ctx)\n",
    "    para_two_B._init_impl(conv_two_B,ctx)\n",
    "    #para_two_W.weight=conv_two_W\n",
    "    #para_two_B.weight=conv_two_B\n",
    "    \n",
    "    paraDict._params['conv_two_W']=para_two_W\n",
    "    paraDict._params['conv_two_B']=para_two_B\n",
    "        \n",
    "    para_three_W=gluon.Parameter('conv_three_W',shape=conv_three_W.shape,init=init.Xavier())\n",
    "    para_three_B=gluon.Parameter('conv_three_B',shape=conv_three_B.shape,init=init.Xavier())\n",
    "    #para_three_W.initialize()\n",
    "    #para_three_B.initialize()\n",
    "    \n",
    "    #para_three_W.weight=conv_three_W\n",
    "    #para_three_B.weight=conv_three_B\n",
    "    para_three_W._init_impl(conv_three_W,ctx)\n",
    "    para_three_B._init_impl(conv_three_B,ctx)\n",
    "    \n",
    "    paraDict._params['conv_three_W']=para_three_W\n",
    "    paraDict._params['conv_three_B']=para_three_B\n",
    "        \n",
    "    para_four_W=gluon.Parameter('conv_four_W',shape=conv_four_W.shape,init=init.Xavier())\n",
    "    para_four_B=gluon.Parameter('conv_four_B',shape=conv_four_B.shape)\n",
    "    #para_four_W.initialize()\n",
    "    #para_four_B.initialize()\n",
    "    para_four_W._init_impl(conv_four_W,ctx)\n",
    "    para_four_B._init_impl(conv_four_B,ctx)\n",
    "    #para_four_W.weight=conv_four_W\n",
    "    #para_four_B.weight=conv_four_B\n",
    "    \n",
    "    paraDict._params['conv_four_W']=para_four_W\n",
    "    paraDict._params['conv_four_B']=para_four_B\n",
    "        \n",
    "    para_full_connect_W=gluon.Parameter('full_connect_W',shape=full_connect_W.shape,init=init.Xavier())\n",
    "    para_full_connect_B=gluon.Parameter('full_connect_B',shape=full_connect_B.shape,init=init.Xavier())\n",
    "    #para_full_connect_W.initialize()\n",
    "    #para_full_connect_B.initialize()\n",
    "    \n",
    "    #para_full_connect_W.weight=full_connect_W\n",
    "    #para_full_connect_B.weight=full_connect_B\n",
    "    para_full_connect_W._init_impl(full_connect_W,ctx)\n",
    "    para_full_connect_B._init_impl(full_connect_B,ctx)\n",
    "    \n",
    "    paraDict._params['full_connect_W']=para_full_connect_W\n",
    "    paraDict._params['full_connect_B']=para_full_connect_B\n",
    "    return paraDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "915cfc68-3e53-4baa-8da9-0a450354733a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNCodeRecheckTrainModel(nn.Block):\n",
    "    def __init__(self,vocab_size,embed_size,embedding_weight,**kwargs):\n",
    "        super(CNNCodeRecheckTrainModel,self).__init__(**kwargs)\n",
    "        #参与训练嵌入层\n",
    "        ctx=d2l.try_gpu()\n",
    "        self.embedding=nn.Embedding(input_dim=vocab_size,output_dim=embed_size)\n",
    "        self.embedding.weight.initialize(init=init.Xavier(),force_reinit=True,ctx=ctx)\n",
    "        self.embedding.weight.set_data(embedding_weight)\n",
    "        #self.ebmedding.weight=self.embedding.weight.as_in_cntexts(ctx)\n",
    "        #不参与训练嵌入层\n",
    "        #self.constant_embedding=nn.Embedding(input_dim=vocab_size,output_dim=embed_size)\n",
    "        #申请梯度\n",
    "        params=read_params('data/params/conv/')\n",
    "        conv_one_W=params[0][0].as_in_context(ctx)\n",
    "        conv_one_B=params[1][0].as_in_context(ctx)\n",
    "        conv_two_W=params[2][0].as_in_context(ctx)\n",
    "        conv_two_B=params[3][0].as_in_context(ctx)\n",
    "        conv_three_W=params[4][0].as_in_context(ctx)\n",
    "        conv_three_B=params[5][0].as_in_context(ctx)\n",
    "        conv_four_W=params[6][0].as_in_context(ctx)\n",
    "        conv_four_B=params[7][0].as_in_context(ctx)\n",
    "        full_connect_W=params[8][0].as_in_context(ctx)\n",
    "        full_connect_B=params[9][0].as_in_context(ctx)\n",
    "        params=[conv_one_W,conv_one_B,conv_two_W,conv_two_B,conv_three_W,conv_three_B,conv_four_W,conv_four_B,full_connect_W,full_connect_B]\n",
    "        for param in params:\n",
    "            #print(param)\n",
    "            param.attach_grad()\n",
    "        self.paraDict=make_parameterDict(params)\n",
    "      \n",
    "        self.conv_one_W=conv_one_W\n",
    "        #self.conv_one_W=self.paraDict['conv_one_W'].data()\n",
    "        self.conv_one_B=conv_one_B\n",
    "        #self.conv_one_B=self.paraDict['conv_one_B'].weight\n",
    "        self.conv_two_W=conv_two_W\n",
    "        #self.conv_two_W=self.paraDict[\"conv_two_W\"].weight\n",
    "        self.conv_two_B=conv_two_B\n",
    "        #self.conv_two_B=self.paraDict[\"conv_two_B\"].weight\n",
    "        self.conv_three_W=conv_three_W\n",
    "        #self.conv_three_W=self.paraDict[\"conv_three_W\"].weight\n",
    "        self.conv_three_B=conv_three_B\n",
    "        #self.conv_three_B=self.paraDict[\"conv_three_B\"].weight\n",
    "        self.conv_four_W=conv_four_W\n",
    "        #self.conv_four_W=self.paraDict[\"conv_four_W\"].weight\n",
    "        self.conv_four_B=conv_four_B\n",
    "        #self.conv_four_B=self.paraDict[\"conv_four_B\"].weight\n",
    "        self.full_connect_W=full_connect_W\n",
    "        #self.full_connect_W=self.paraDict[\"full_connect_W\"].weight\n",
    "        self.full_connect_B=full_connect_B\n",
    "        #self.full_connect_B=self.paraDict[\"full_connect_B\"].weight\n",
    "        self.conv_layer=conv_layer\n",
    "        self.max_pool_layer=max_pool_layer\n",
    "        self.full_connect_layer=full_connect_layer\n",
    "        self.dropout_layer=dropout_layer\n",
    "        #self.paraDict=make_parameterDict(params)  \n",
    "        \n",
    "    def set_embedding_weight(self,embedding_weight):\n",
    "        self.embedding.weight.set_data(embedding_weight)\n",
    "    def forward(self,inputs,mode='train'):\n",
    "        ctx=d2l.try_gpu()\n",
    "        inputs_embeddings=nd.array([],ctx=ctx)\n",
    "        if mode=='train':\n",
    "            inputs_embeddings=nd.stack(nd.flatten(self.embedding(inputs[0])),nd.flatten(self.embedding(inputs[1])),nd.flatten(self.embedding(inputs[2])))\n",
    "        else :\n",
    "            inputs_embeddings=nd.stack(nd.flatten(self.embedding(inputs[0])),nd.flatten(self.embedding(inputs[1])))\n",
    "        #print(inputs_embeddings)\n",
    "        #input_embedding=inputs_embeddings[0]\n",
    "        #sum=0;\n",
    "        results=[]\n",
    "        result_array=nd.array([],ctx=ctx)\n",
    "        for input_embedding in inputs_embeddings:\n",
    "            input_embedding=input_embedding.as_in_context(ctx)\n",
    "            result=conv_layer(input_embedding,3,self.conv_one_W,self.conv_one_B,ctx)\n",
    "            result=result.reshape(-1,9)\n",
    "            #print(result)\n",
    "            result=max_pool_layer(result,ctx)\n",
    "            result=result.reshape(-1,9)\n",
    "            #print(result)\n",
    "            result=conv_layer(result,3,self.conv_two_W,self.conv_two_B,ctx)\n",
    "            result=result.reshape(-1,9)\n",
    "            \n",
    "            result=max_pool_layer(result,ctx)\n",
    "            result=result.reshape(-1,9)\n",
    "            result=conv_layer(result,3,self.conv_three_W,self.conv_three_B,ctx)\n",
    "            result=result.reshape(-1,9)\n",
    "            \n",
    "            result=max_pool_layer(result,ctx)\n",
    "            result=result.reshape(-1,9)\n",
    "            result=conv_layer(result,1,self.conv_four_W,self.conv_four_B,ctx)\n",
    "            result=result.reshape(-1,1)\n",
    "            \n",
    "            result=full_connect_layer(result,self.full_connect_W,self.full_connect_B,ctx)\n",
    "            results.append(result)\n",
    "        #print(len(results))\n",
    "        if len(results)==3:\n",
    "            result_array=nd.stack(results[0],results[1],results[2])\n",
    "        else :\n",
    "            result_array=nd.stack(results[0],results[1])\n",
    "        return result_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "4d5cafe2-b98b-44b9-a9ac-564dd1b9cb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mult_file_data(sourcePath,negativePath,positivePath,token_to_idx,text_size):\n",
    "    text=[]\n",
    "    ctx=d2l.try_gpu()\n",
    "    sourceText=read_file_data(sourcePath,token_to_idx,text_size)\n",
    "    negativeText=read_file_data(negativePath,token_to_idx,text_size)\n",
    "    positiveText=read_file_data(positivePath,token_to_idx,text_size)\n",
    "    #text=nd.stack(sourceText,positiveText,negativeText)\n",
    "    sourceText=nd.array(sourceText,ctx=ctx)\n",
    "    positiveText=nd.array(positiveText,ctx=ctx)\n",
    "    negativeText=nd.array(negativeText,ctx=ctx)\n",
    "    text=nd.stack(sourceText,positiveText,negativeText)\n",
    "    return text\n",
    "def get_fileNames(dirPath):\n",
    "    filenameSet=[]\n",
    "    for dirpath,dirnames,filenames in os.walk(dirPath):\n",
    "        for fileaname in filenames:\n",
    "            filePath=os.path.join(dirpath,fileaname)\n",
    "            if os.path.isfile(filePath):\n",
    "                if filePath.endswith(\".txt\"):\n",
    "                       filenameSet.append(fileaname)\n",
    "    #print(filenameSet)\n",
    "    sorted(filenameSet)\n",
    "    return filenameSet\n",
    "def get_all_file_in_dir(dirPath):\n",
    "    filePaths=[]\n",
    "    for dirpath,dirnames,filenames in os.walk(dirPath):\n",
    "        for filename in filenames:\n",
    "           # filenameSet.append(filenames)\n",
    "            #print(os.path.join(dirpath,filename))\n",
    "            filePath=os.path.join(dirpath,filename)\n",
    "            if os.path.isfile(filePath):\n",
    "                if filePath.endswith(\".txt\"):\n",
    "                    #print(filePath)\n",
    "                    filePaths.append(filePath)\n",
    "    sorted(filePaths)\n",
    "    return filePaths\n",
    "def load_train_data(net,token_to_idx,text_size):\n",
    "    #filenames=get_fileNames(\"C:/Users/lwq/Desktop/trainData/source\")\n",
    "    #print(filenames)\n",
    "    sourceFiles=get_all_file_in_dir(\"C:/Users/lwq/Desktop/trainData/source\")\n",
    "    positiveFiles=get_all_file_in_dir(\"C:/Users/lwq/Desktop/trainData/positive\")\n",
    "    negativeFiles=get_all_file_in_dir(\"C:/Users/lwq/Desktop/trainData/negative\")\n",
    "    texts=[]\n",
    "    for idx in range(len(sourceFiles)):\n",
    "        #print(sourceFiles[idx])\n",
    "        #print(negativeFiles[idx])\n",
    "        #print(positiveFiles[idx])\n",
    "        negativeFiles_idx=negativeFiles.index(sourceFiles[idx].replace('source','negative'))\n",
    "        positiveFiles_idx=positiveFiles.index(sourceFiles[idx].replace('source','positive'))\n",
    "        #print(sourceFiles[idx])\n",
    "        #print(negativeFiles[negativeFiles_idx])\n",
    "        #print(positiveFiles[positiveFiles_idx])\n",
    "        text=get_mult_file_data(sourceFiles[idx],negativeFiles[negativeFiles_idx],positiveFiles[positiveFiles_idx],token_to_idx,text_size)\n",
    "        #print(text)\n",
    "        texts.append(text)\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d89706da-8a74-495f-a47d-f16a4417d3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_calculed_data(filePath,data):\n",
    "    nd.save(filePath,data)\n",
    "def read_calculed_data(filePath):\n",
    "    data=nd.load(filePath)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "88d83d28-8f99-465c-8a31-a99258c1fa30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_text_embeddings(net,texts):\n",
    "    print('train set train')\n",
    "    text_embeddings=[]\n",
    "    sum=0\n",
    "    time_total=0\n",
    "    text_total=len(texts)\n",
    "    for text in texts:\n",
    "        #print(text)\n",
    "        time_start=time.time()\n",
    "        embedd=net(text)\n",
    "        text_embeddings.append(embedd)\n",
    "        time_end=time.time()\n",
    "        cost_time=time_end-time_start\n",
    "        time_total+=cost_time\n",
    "        print('样本%d/%d计算花费时间%.3fs'%(sum,text_total,cost_time))\n",
    "        sum+=1\n",
    "    minus=time_total/60\n",
    "    time_total=time_total-minus*60\n",
    "    print('%d份样本花费时间%d分%.3f'%(sum,minus,time_total))\n",
    "    return text_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "cf8c9d40-a481-4f50-b88e-a99d7cf1ca82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_batch(batch,ctx):\n",
    "    X=batch\n",
    "    return (gutils.split_and_load(X,ctx),X.shape[0])\n",
    "def evaluate_accuracy(data_iter,net,ctx=d2l.try_gpu()):\n",
    "    acc_sum,n=nd.array([0]),0\n",
    "    for batch in data_iter:\n",
    "        features,labels,_=_get_batch(batch,ctx)\n",
    "        for X,y in zip(features,labels):\n",
    "            y=y.astype('float32')\n",
    "            acc_sum+=(net(X).argmax(axis=1)==y).sum().copy_to(d2l.try_gpu())\n",
    "            n+=y.size\n",
    "        acc_sum.wait.to_read()\n",
    "        return acc_sum.asscalar()/n\n",
    "def simi(t_one,t_two):\n",
    "    cos=nd.dot(t_one.T,t_two).sum()/(nd.dot(t_one.T,t_one).sum()*nd.dot(t_two.T,t_two).sum()+1e-9).sqrt()\n",
    "    return cos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "5aca9f0d-4cd4-486b-a627-800827a41863",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Block):\n",
    "    def __init__(self,vocab_size,embed_size,kernel_sizes,num_channels,**kwargs):\n",
    "        super(CNN,self).__init__(**kwargs)\n",
    "        self.embedding=nn.Embedding(vocab_size,embed_size)\n",
    "        self.pool=nn.MaxPool1D(pool_size=2,strides=2)\n",
    "        self.conv_one=nn.Conv1D(channels=3,kernel_size=3,strides=1,activation='relu')\n",
    "        self.conv_two=nn.Conv1D(channels=3,kernel_size=3,strides=1,activation='relu')\n",
    "        self.conv_three=nn.Conv1D(channels=2,kernel_size=3,strides=1,activation='relu')\n",
    "        #self.conv_four=nn.Conv1D(channels=1,kernel_size=3,strides=1,activation='relu')\n",
    "        self.dropout=nn.Dropout(0.2) \n",
    "        self.dense=nn.Dense(10,activation='tanh')\n",
    "    def forward(self,inputs):\n",
    "        results=[]\n",
    "        for _input in inputs:\n",
    "            embeddings=nd.flatten(self.embedding(_input))\n",
    "            embeddings=nd.stack(embeddings)\n",
    "            embeddings=embeddings.reshape((8,102,20))\n",
    "            embeddings=embeddings.transpose((0,2,1))\n",
    "            #print(embeddings)\n",
    "            conv_one=self.conv_one(embeddings)\n",
    "            #print(conv_one)\n",
    "            #print(self.conv_one.bias.data())\n",
    "            #print(self.conv_two.bias.data())\n",
    "            pool_one=self.pool(conv_one)\n",
    "            #ctx=d2l.try_gpu()\n",
    "            conv_two=self.conv_two(pool_one)\n",
    "            pool_two=self.pool(conv_two)\n",
    "            conv_three=self.conv_three(pool_two)\n",
    "            pool_three=self.pool(conv_three)\n",
    "            dropout=self.dropout(pool_three)\n",
    "            dropout=dropout.flatten()\n",
    "            dense=self.dense(dropout)\n",
    "            dense=dense.reshape((-1,1))\n",
    "            results.append(dense)\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "5b060382-30c2-4c24-b104-6e0e48bd571a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_iter,test_iter,net,trainer,ctx,num_epochs):\n",
    "    print('training on',ctx)\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum,train_acc_sum,n,m,start=0.0,0.0,0,0,time.time()\n",
    "        for i,batch in enumerate(train_iter):\n",
    "            Xs,batch_size=_get_batch(batch,ctx)\n",
    "            ls=[]\n",
    "            Xs=Xs[0]\n",
    "            #print(Xs)\n",
    "            cot=0\n",
    "            total_time=0\n",
    "            print(\"batch:%d,batch_size:%d\"%(i,batch_size))\n",
    "            time_start=time.time()\n",
    "            with autograd.record():\n",
    "                y_hats=[net(X) for X in Xs]\n",
    "                #print(y_hats)\n",
    "                for y_hat in y_hats:\n",
    "                    simi_one=simi(y_hat[0],y_hat[1])\n",
    "                    simi_two=simi(y_hat[0],y_hat[2])\n",
    "                    val=max(0,simi_two-simi_one+1)\n",
    "                    ls.append(val)\n",
    "            #print(ls)\n",
    "            for l in ls:\n",
    "                if l!=0:\n",
    "                    l.backward()\n",
    "            trainer.step(batch_size)\n",
    "            time_end=time.time()\n",
    "            #total_time=time_end-time_start\n",
    "            #minus=total_time/60\n",
    "            #total_time=total_time-minus*60\n",
    "            print(\"batch_size %d 用时%.3f秒\"%(batch_size,time_end-time_start))\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "8493c44c-5c56-47dd-b759-32127a8c6111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "258\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "[[-0.08890261 -0.1549949   0.11318387 ... -0.05622941 -0.10188581\n",
       "  -0.09949586]\n",
       " [ 0.23858748 -0.21557562  0.23247422 ... -0.0104394  -0.19908598\n",
       "  -0.05888294]\n",
       " [-0.13100792 -0.06095976 -0.14211853 ...  0.05129166 -0.02374929\n",
       "   0.19797756]\n",
       " ...\n",
       " [-0.1187419  -0.20316416 -0.1586011  ... -0.23782516  0.04215746\n",
       "   0.13563322]\n",
       " [ 0.20655583  0.23223405  0.00860475 ... -0.11362076 -0.18347228\n",
       "   0.15392269]\n",
       " [ 0.1477852   0.03926985  0.12885992 ...  0.05460896 -0.12567274\n",
       "   0.04938133]]\n",
       "<NDArray 80x20 @gpu(0)>"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctx=d2l.try_gpu()\n",
    "embed_size=80\n",
    "lr,num_epochs=0.001,5\n",
    "net=CNN(len(idx_to_token),embedding_dim,kernel_sizes,nums_channels)\n",
    "net.initialize(init.Xavier(),ctx=ctx)\n",
    "result=net(text)\n",
    "trainer=gluon.Trainer(net.collect_params(),'adam',{'learning_rate':lr})\n",
    "texts=load_train_data(net,token_to_idx,text_size)\n",
    "print(len(texts))\n",
    "test_text_embeddings=[]\n",
    "batch_size=256\n",
    "train_iter=gdata.DataLoader(gdata.ArrayDataset(texts),batch_size,shuffle=True)\n",
    "test_iter=gdata.DataLoader(gdata.ArrayDataset(test_text_embeddings),batch_size,shuffle=True)\n",
    "ctx=d2l.try_all_gpus()\n",
    "net.collect_params()\n",
    "net.embedding.weight.data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "0b5db652-2659-4385-801f-44a0b2eecfcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on [gpu(0)]\n",
      "batch:0,batch_size:256\n",
      "batch_size 256 用时1.600秒\n",
      "batch:1,batch_size:2\n",
      "batch_size 2 用时0.513秒\n",
      "batch:0,batch_size:256\n",
      "batch_size 256 用时1.501秒\n",
      "batch:1,batch_size:2\n",
      "batch_size 2 用时0.515秒\n",
      "batch:0,batch_size:256\n",
      "batch_size 256 用时1.418秒\n",
      "batch:1,batch_size:2\n",
      "batch_size 2 用时0.481秒\n",
      "batch:0,batch_size:256\n",
      "batch_size 256 用时1.357秒\n",
      "batch:1,batch_size:2\n",
      "batch_size 2 用时0.477秒\n",
      "batch:0,batch_size:256\n",
      "batch_size 256 用时1.374秒\n",
      "batch:1,batch_size:2\n",
      "batch_size 2 用时0.462秒\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "[[-0.09379651 -0.15117992  0.11749113 ... -0.06223327 -0.1064942\n",
       "  -0.09442525]\n",
       " [ 0.23994872 -0.21664682  0.23142111 ... -0.00861475 -0.19691354\n",
       "  -0.06135642]\n",
       " [-0.13127178 -0.0584424  -0.13837902 ...  0.05407244 -0.02335997\n",
       "   0.19882755]\n",
       " ...\n",
       " [-0.1187419  -0.20316416 -0.1586011  ... -0.23782516  0.04215746\n",
       "   0.13563322]\n",
       " [ 0.20655583  0.23223405  0.00860475 ... -0.11362076 -0.18347228\n",
       "   0.15392269]\n",
       " [ 0.1477852   0.03926985  0.12885992 ...  0.05460896 -0.12567274\n",
       "   0.04938133]]\n",
       "<NDArray 80x20 @gpu(0)>"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(train_iter,test_iter,net,trainer,ctx,num_epochs)\n",
    "net.embedding.weight.data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "480d21f1-abfe-4fb6-8a17-6407df2acedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.save_parameters(\"data/params/model.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "43fe2609-80c7-490f-955e-f3dde174b92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.load_parameters(\"data/params/model.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8223505-a48e-414f-a6e7-259ee5f52bc7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
